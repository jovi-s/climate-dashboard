{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m div_table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, attrs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable-container\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Extract all rows from the div table\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[43mdiv_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Initialize an empty list to store hrefs\u001b[39;00m\n\u001b[1;32m     23\u001b[0m href_list \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
    "}\n",
    "\n",
    "base_link=\"https://unfccc.int/first-biennial-transparency-reports\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(base_link, headers=headers)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the div containing the table\n",
    "div_table = soup.find('div', attrs = {'class': 'table-container'})\n",
    "\n",
    "# Extract all rows from the div table\n",
    "rows = div_table.find_all('tr')\n",
    "\n",
    "# Initialize an empty list to store hrefs\n",
    "href_list = []\n",
    "\n",
    "# Iterate over each row and extract data\n",
    "for row in rows:\n",
    "    # Extract all columns in the current row with the specific style\n",
    "    columns = row.find_all('td', style='vertical-align:top; width:177px')\n",
    "    for column in columns:\n",
    "        # Find all <p> elements within the column\n",
    "        paragraphs = column.find_all('p')\n",
    "        for paragraph in paragraphs:\n",
    "            # Check if the paragraph contains the text \"BTR\"\n",
    "            if 'BTR' in paragraph.get_text():\n",
    "                # Find all <a> tags within the paragraph\n",
    "                links = paragraph.find_all('a')\n",
    "                for link in links:\n",
    "                    # Get the href attribute\n",
    "                    href = link.get('href')\n",
    "                    # Append the href to the list\n",
    "                    href_list.append(href)\n",
    "\n",
    "# Print the list of hrefs\n",
    "print(href_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(div_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html style=\"height:100%\">\n",
      " <head>\n",
      "  <meta content=\"NOINDEX, NOFOLLOW\" name=\"ROBOTS\"/>\n",
      "  <meta content=\"telephone=no\" name=\"format-detection\"/>\n",
      "  <meta content=\"initial-scale=1.0\" name=\"viewport\"/>\n",
      "  <meta content=\"IE=edge,chrome=1\" http-equiv=\"X-UA-Compatible\"/>\n",
      " </head>\n",
      " <body style=\"margin:0px;height:100%\">\n",
      "  <iframe frameborder=\"0\" height=\"100%\" id=\"main-iframe\" marginheight=\"0px\" marginwidth=\"0px\" src=\"/_Incapsula_Resource?SWUDNSAI=31&amp;xinfo=13-349259877-0%202NNN%20RT%281735709275449%20193%29%20q%280%20-1%20-1%20-1%29%20r%280%20-1%29%20B12%284%2c316%2c0%29&amp;incident_id=260000130640992918-1520974852994106189&amp;edet=12&amp;cinfo=04000000&amp;rpinfo=0&amp;cts=PNhRgPQY1X4en2DEn0QRahTCQilpuGXbqXRyPTL2dVI9lxzRU53mu3yDTRYz6WHz&amp;mth=GET\" width=\"100%\">\n",
      "   Request unsuccessful. Incapsula incident ID: 260000130640992918-1520974852994106189\n",
      "  </iframe>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html style=\"height:100%\"><head><META NAME=\"ROBOTS\" CONTENT=\"NOINDEX, NOFOLLOW\"><meta name=\"format-detection\" content=\"telephone=no\"><meta name=\"viewport\" content=\"initial-scale=1.0\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\"></head><body style=\"margin:0px;height:100%\"><iframe id=\"main-iframe\" src=\"/_Incapsula_Resource?SWUDNSAI=31&xinfo=13-349306138-0%202NNN%20RT%281735709549740%20292%29%20q%280%20-1%20-1%20-1%29%20r%280%20-1%29%20B12%284%2c316%2c0%29&incident_id=260000130640992918-1521187827537412941&edet=12&cinfo=04000000&rpinfo=0&cts=Ht0QstEPPJWuZasV91d7E1Jp2uu8fuObUGs%2f2MalcR2BRk6dL2cbLngDT5uzCiS0&mth=GET\" frameborder=0 width=\"100%\" height=\"100%\" marginheight=\"0px\" marginwidth=\"0px\">Request unsuccessful. Incapsula incident ID: 260000130640992918-1521187827537412941</iframe></body></html>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(base_link).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test RAG\n",
    "\n",
    "1. [LlamaIndex DocSummary](https://docs.llamaindex.ai/en/stable/examples/index_structs/doc_summary/DocSummary/)\n",
    "2. [Semi_structured_and_multi_modal_RAG](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb)\n",
    "3. [multi_modal_RAG_chroma](https://github.com/langchain-ai/langchain/blob/master/cookbook/multi_modal_RAG_chroma.ipynb)\n",
    "3. [Semi_Structured_RAG](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb)\n",
    "3. [CrewAI PDFSearch Tool](https://docs.crewai.com/tools/pdfsearchtool#pdfsearchtool)\n",
    "4. [Advanced RAG with LlamaParse](https://docs.llamaindex.ai/en/stable/examples/cookbooks/oreilly_course_cookbooks/Module-8/Advanced_RAG_with_LlamaParse/)\n",
    "7. [LangGraph Retrieval Agent](https://github.com/langchain-ai/langchain/blob/master/cookbook/langgraph_agentic_rag.ipynb)\n",
    "\n",
    "\n",
    "Improvements: \n",
    "1. Query Transformation, (RAG Fusion?)\n",
    "2. Text, Table Multi Vector Retriever, \n",
    "3. LangGraph Agent RAG\n",
    "4. ReRanker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install markitdown langchain-chroma langchain_community tiktoken langchain-openai langchainhub langchain langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/josingh/hobby/climate-dashboard/\")\n",
    "\n",
    "btr_file_path = \"data/btr/Singapore%20BTR1%202024.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markitdown Extraction\n",
    "\n",
    "[markitdown](https://github.com/microsoft/markitdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from markitdown import MarkItDown\n",
    "\n",
    "md = MarkItDown()\n",
    "result = md.convert(btr_file_path)\n",
    "# print(result.text_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LlamaParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-index-postprocessor-flag-embedding-reranker\n",
    "!pip install git+https://github.com/FlagOpen/FlagEmbedding.git\n",
    "!pip install llama-parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama-parse is async-first, running the async code in a notebook requires the use of nest_asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 3bd10bf8-1c99-4ba2-94f8-36576a34b06d\n",
      "........."
     ]
    }
   ],
   "source": [
    "# LlamaParse PDF reader for PDF Parsing\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "documents = LlamaParse(result_type=\"markdown\").load_data(\n",
    "    \"./data/btr/Singapore%20BTR1%202024.pdf\"\n",
    ")\n",
    "# Started parsing the file under job_id 3bd10bf8-1c99-4ba2-94f8-36576a34b06d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "2it [00:00, 48489.06it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "1it [00:00, 11491.24it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "2it [00:00, 27776.85it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "1it [00:00, 16070.13it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "1it [00:00, 28532.68it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "1it [00:00, 16131.94it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "1it [00:00, 11125.47it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "1it [00:00, 14979.66it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 19691.57it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "2it [00:00, 41734.37it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 23831.27it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 20867.18it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "2it [00:00, 31775.03it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "2it [00:00, 41734.37it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "2it [00:00, 31300.78it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "1it [00:00, 19239.93it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "3it [00:00, 26546.23it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "2it [00:00, 27413.75it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "4it [00:00, 76959.71it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "2it [00:00, 45343.83it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "1it [00:00, 15650.39it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "2it [00:00, 51150.05it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "2it [00:00, 33420.75it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "4it [00:00, 50533.78it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "2it [00:00, 55553.70it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "2it [00:00, 45343.83it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "2it [00:00, 47662.55it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "3it [00:00, 65196.44it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "3it [00:00, 56425.61it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "2it [00:00, 30840.47it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 6574.14it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "1it [00:00, 14716.86it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "2it [00:00, 35696.20it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 19972.88it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "1it [00:00, 27962.03it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "1it [00:00, 12710.01it/s]\n",
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:110: DeprecationWarning: Call to deprecated method get_doc_id. ('get_doc_id' is deprecated, access the 'id_' property instead.) -- Deprecated since version 0.12.2.\n",
      "  docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import MarkdownElementNodeParser\n",
    "\n",
    "node_parser = MarkdownElementNodeParser(\n",
    "    llm=OpenAI(model=\"gpt-4o-mini\"), num_workers=8\n",
    ")\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_nodes, index_nodes = node_parser.get_nodes_and_objects(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_index = VectorStoreIndex(nodes=text_nodes + index_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load\n",
    "from llama_index.core import load_index_from_storage, StorageContext\n",
    "\n",
    "country = \"Singapore\"\n",
    "# if not os.path.exists(f\"./data/vector_store/btr/{country}\"):\n",
    "#     # build vector index\n",
    "#     recursive_index.storage_context.persist(\n",
    "#         persist_dir=f\"./data/vector_store/btr/{country}\"\n",
    "#     )\n",
    "# else:\n",
    "recursive_index = load_index_from_storage(\n",
    "    StorageContext.from_defaults(persist_dir=f\"./data/vector_store/btr/{country}\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josingh/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from llama_index.postprocessor.flag_embedding_reranker import (\n",
    "#     FlagEmbeddingReranker,\n",
    "# )\n",
    "\n",
    "# reranker = FlagEmbeddingReranker(\n",
    "#     top_n=5,\n",
    "#     model=\"BAAI/bge-reranker-large\",\n",
    "# )\n",
    "\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model=\"models/rerank\", top_n=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_query_engine = recursive_index.as_query_engine(\n",
    "    similarity_top_k=15, node_postprocessors=[reranker], verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;11;159;203mRetrieval entering e38c4afd-d0ed-4ef3-95eb-fcbfe4240e1c: TextNode\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What are the list of mitigation measures pertaining to shifting to cleaner energy sources?\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering e2e0c5de-6b6f-4667-8df3-a08e2efc8de4: TextNode\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What are the list of mitigation measures pertaining to shifting to cleaner energy sources?\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering edb9663d-1bcf-4e55-9bc7-13ce2285244a: TextNode\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What are the list of mitigation measures pertaining to shifting to cleaner energy sources?\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering bbbbb3c6-c90b-4a30-97b9-64c831328b96: TextNode\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What are the list of mitigation measures pertaining to shifting to cleaner energy sources?\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering 8c0bd701-7b98-44ba-8ff1-b9e93a624917: TextNode\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What are the list of mitigation measures pertaining to shifting to cleaner energy sources?\n",
      "\u001b[0mThe list of mitigation measures pertaining to shifting to cleaner energy sources includes implementing a carbon tax, progressively increasing the tax rate over time, providing an economy-wide price signal to encourage emission reductions, and taking the most economical action where it makes sense.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the list of mitigation measures pertaining to shifting to cleaner energy sources?\"\n",
    "# \"ccc\"\n",
    "# Explain how Singapore is Alternative Energy Disadvantaged?\n",
    "\n",
    "response_2 = recursive_query_engine.query(query)\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph Retrieval Agent\n",
    "\n",
    "[langgraph_agentic_rag](https://github.com/langchain-ai/langchain/blob/master/cookbook/langgraph_agentic_rag.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(btr_file_path)\n",
    "docs_list = []\n",
    "for page in loader.lazy_load():\n",
    "    docs_list.append(page)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qq/k9cdttkj35z90gb0sk_tqw1m0000gp/T/ipykernel_21530/2567641636.py:13: LangGraphDeprecationWarning: ToolExecutor is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
      "  tool_executor = ToolExecutor(tools)\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_biennial_transparency_report\",\n",
    "    \"Search and return information about biennial transparency reports (BTR).\",\n",
    ")\n",
    "\n",
    "tools = [tool]\n",
    "\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "tool_executor = ToolExecutor(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import ToolInvocation\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def should_retrieve(state):\n",
    "    \"\"\"\n",
    "    Decides whether the agent should retrieve more information or end the process.\n",
    "\n",
    "    This function checks the last message in the state for a function call. If a function call is\n",
    "    present, the process continues to retrieve information. Otherwise, it ends the process.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state of the agent, including all messages.\n",
    "\n",
    "    Returns:\n",
    "        str: A decision to either \"continue\" the retrieval process or \"end\" it.\n",
    "    \"\"\"\n",
    "    print(\"---DECIDE TO RETRIEVE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if \"function_call\" not in last_message.additional_kwargs:\n",
    "        print(\"---DECISION: DO NOT RETRIEVE / DONE---\")\n",
    "        return \"end\"\n",
    "    # Otherwise there is a function call, so we continue\n",
    "    else:\n",
    "        print(\"---DECISION: RETRIEVE---\")\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "def check_relevance(state):\n",
    "    \"\"\"\n",
    "    Determines whether the Agent should continue based on the relevance of retrieved documents.\n",
    "\n",
    "    This function checks if the last message in the conversation is of type FunctionMessage, indicating\n",
    "    that document retrieval has been performed. It then evaluates the relevance of these documents to the user's\n",
    "    initial question using a predefined model and output parser. If the documents are relevant, the conversation\n",
    "    is considered complete. Otherwise, the retrieval process is continued.\n",
    "\n",
    "    Args:\n",
    "        state messages: The current state of the conversation, including all messages.\n",
    "\n",
    "    Returns:\n",
    "        str: A directive to either \"end\" the conversation if relevant documents are found, or \"continue\" the retrieval process.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Output\n",
    "    class FunctionOutput(BaseModel):\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # Create an instance of the PydanticOutputParser\n",
    "    parser = PydanticOutputParser(pydantic_object=FunctionOutput)\n",
    "\n",
    "    # Get the format instructions from the output parser\n",
    "    format_instructions = parser.get_format_instructions()\n",
    "\n",
    "    # Create a prompt template with format instructions and the query\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of retrieved docs to a user question. \\n \n",
    "        Here are the retrieved docs:\n",
    "        \\n ------- \\n\n",
    "        {context} \n",
    "        \\n ------- \\n\n",
    "        Here is the user question: {question}\n",
    "        If the docs contain keyword(s) in the user question, then score them as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the docs are relevant to the question. \\n \n",
    "        Output format instructions: \\n {format_instructions}\"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "        partial_variables={\"format_instructions\": format_instructions},\n",
    "    )\n",
    "\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "\n",
    "    chain = prompt | model | parser\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    score = chain.invoke(\n",
    "        {\"question\": messages[0].content, \"context\": last_message.content}\n",
    "    )\n",
    "\n",
    "    # If relevant\n",
    "    if score.binary_score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"yes\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score.binary_score)\n",
    "        return \"no\"\n",
    "\n",
    "\n",
    "### Nodes\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state.\n",
    "\n",
    "    This function calls the agent model to generate a response to the current conversation state.\n",
    "    The response is added to the state's messages.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state of the agent, including all messages.\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the new message added to the list of messages.\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    model = ChatOpenAI(temperature=0, streaming=True, model=\"gpt-4o-mini\")\n",
    "    functions = [format_tool_to_openai_function(t) for t in tools]\n",
    "    model = model.bind_functions(functions)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define the function to execute tools\n",
    "def call_tool(state):\n",
    "    \"\"\"\n",
    "    Executes a tool based on the last message's function call.\n",
    "\n",
    "    This function is responsible for executing a tool invocation based on the function call\n",
    "    specified in the last message. The result from the tool execution is added to the conversation\n",
    "    state as a new message.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state of the agent, including all messages.\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the new function message added to the list of messages.\n",
    "    \"\"\"\n",
    "    print(\"---EXECUTE RETRIEVAL---\")\n",
    "    messages = state[\"messages\"]\n",
    "    # Based on the continue condition\n",
    "    # we know the last message involves a function call\n",
    "    last_message = messages[-1]\n",
    "    # We construct an ToolInvocation from the function_call\n",
    "    action = ToolInvocation(\n",
    "        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
    "        tool_input=json.loads(\n",
    "            last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "        ),\n",
    "    )\n",
    "    # We call the tool_executor and get back a response\n",
    "    response = tool_executor.invoke(action)\n",
    "    # print(type(response))\n",
    "    # We use the response to create a FunctionMessage\n",
    "    function_message = FunctionMessage(content=str(response), name=action.tool)\n",
    "\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [function_message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x309573ef0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)  # agent\n",
    "workflow.add_node(\"action\", call_tool)  # retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call agent node to decide to retrieve or not\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    # Assess agent decision\n",
    "    should_retrieve,\n",
    "    {\n",
    "        # Call tool node\n",
    "        \"continue\": \"action\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"action\",\n",
    "    # Assess agent decision\n",
    "    check_relevance,\n",
    "    {\n",
    "        # Call agent node\n",
    "        \"yes\": \"agent\",\n",
    "        \"no\": END,  # placeholder\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "---DECIDE TO RETRIEVE---\n",
      "---DECISION: RETRIEVE---\n",
      "\"Output from node 'agent':\"\n",
      "'---'\n",
      "{ 'messages': [ AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"Singapore Alternative Energy Disadvantaged\"}', 'name': 'retrieve_biennial_transparency_report'}}, response_metadata={'finish_reason': 'function_call', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-f8cc63fd-9048-4281-9cd3-484974cffb5b-0')]}\n",
      "'\\n---\\n'\n",
      "---EXECUTE RETRIEVAL---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qq/k9cdttkj35z90gb0sk_tqw1m0000gp/T/ipykernel_21530/1634295254.py:154: LangGraphDeprecationWarning: ToolInvocation is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
      "  action = ToolInvocation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK RELEVANCE---\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'FunctionOutput' has no attribute 'model_json_schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 13\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n\u001b[1;32m      5\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m      7\u001b[0m         HumanMessage(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     ]\n\u001b[1;32m     12\u001b[0m }\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m app\u001b[38;5;241m.\u001b[39mstream(inputs):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     15\u001b[0m         pprint\u001b[38;5;241m.\u001b[39mpprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput from node \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1670\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1664\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1665\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1667\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1670\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1671\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1672\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1673\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1674\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1675\u001b[0m         ):\n\u001b[1;32m   1676\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1677\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/langgraph/pregel/runner.py:230\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    228\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:464\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:226\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 226\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/langgraph/graph/graph.py:95\u001b[0m, in \u001b[0;36mBranch._route\u001b[0;34m(self, input, config, reader, writer)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m---> 95\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finish(writer, \u001b[38;5;28minput\u001b[39m, result, config)\n",
      "File \u001b[0;32m~/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:218\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m    217\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m--> 218\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    220\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "Cell \u001b[0;32mIn[21], line 68\u001b[0m, in \u001b[0;36mcheck_relevance\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     65\u001b[0m parser \u001b[38;5;241m=\u001b[39m PydanticOutputParser(pydantic_object\u001b[38;5;241m=\u001b[39mFunctionOutput)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Get the format instructions from the output parser\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m format_instructions \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_format_instructions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Create a prompt template with format instructions and the query\u001b[39;00m\n\u001b[1;32m     71\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m     72\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are a grader assessing relevance of retrieved docs to a user question. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124m    Here are the retrieved docs:\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m     partial_variables\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_instructions\u001b[39m\u001b[38;5;124m\"\u001b[39m: format_instructions},\n\u001b[1;32m     83\u001b[0m )\n",
      "File \u001b[0;32m~/hobby/climate-dashboard/.venv/lib/python3.12/site-packages/langchain_core/output_parsers/pydantic.py:92\u001b[0m, in \u001b[0;36mPydanticOutputParser.get_format_instructions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the format instructions for the JSON output.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m    The format instructions for the JSON output.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Copy schema to avoid altering original Pydantic schema.\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpydantic_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_json_schema\u001b[49m()\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Remove extraneous fields.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m reduced_schema \u001b[38;5;241m=\u001b[39m schema\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'FunctionOutput' has no attribute 'model_json_schema'",
      "\u001b[0mDuring task with name 'action' and id '3f6bc705-9acf-299c-15c4-e83bb4023d3e'"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            # content=\"What are the ways Singapore is reducing its greenhouse gas emissions?\"\n",
    "            content=\"Explain how Singapore is Alternative Energy Disadvantaged from the biennial transparency report\"\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
